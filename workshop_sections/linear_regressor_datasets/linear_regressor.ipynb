{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regression example using the LinearRegressor Estimator and Datasets.\n",
    "\n",
    "First, do some imports and set some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Linear regression using the LinearRegressor Estimator and Datasets.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import datasets_input\n",
    "\n",
    "FLAGS = None\n",
    "PRICE_NORM_FACTOR = 1000\n",
    "# with this simple example, we'll use these two fields for training.\n",
    "TRAIN_FEATURE_NAMES = ['curb-weight','highway-mpg']\n",
    "STEPS=1000\n",
    "# You may need to change the following path on Windows\n",
    "MODEL_DIR = os.path.join(\"/tmp/tfmodels/linear_regressor\", str(int(time.time())))\n",
    "print(\"Using model dir %s\" % MODEL_DIR)\n",
    "\n",
    "# comment this out for less verbosity\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the input Datasets, then build input fns that use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the input Datasets we'll use for training and testing.\n",
    "(train, test) = datasets_input.dataset()\n",
    "\n",
    "# Apply a map() to further transform the data - switch the labels to units\n",
    "# of thousands for better convergence.\n",
    "def to_thousands(features, labels):\n",
    "  return features, labels/PRICE_NORM_FACTOR\n",
    "train = train.map(to_thousands)\n",
    "test = test.map(to_thousands)\n",
    "\n",
    "# Build the training input_fn.\n",
    "def input_train():\n",
    "  return (\n",
    "      # Shuffling with a buffer larger than the data set ensures\n",
    "      # that the examples are well mixed.\n",
    "      train.shuffle(1000).batch(128)\n",
    "      # Repeat forever\n",
    "      .repeat().make_one_shot_iterator().get_next())\n",
    "\n",
    "# Build the validation input_fn.\n",
    "def input_test():\n",
    "  return (test.shuffle(1000).batch(128)\n",
    "          .make_one_shot_iterator().get_next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the feature column info for the training features, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the columns we want to use are numeric, so we can construct\n",
    "# the feature column info like this. In general, different columns might have\n",
    "# different types.\n",
    "feature_columns = [tf.feature_column.numeric_column(key=i) for i in TRAIN_FEATURE_NAMES]\n",
    "\n",
    "# Build the Estimator.\n",
    "model = tf.estimator.LinearRegressor(feature_columns=feature_columns,\n",
    "                                     model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model.\n",
    "# By default, the Estimators log output every 100 steps.\n",
    "model.train(input_fn=input_train, steps=STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll evaluate the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate how the model performs on data it has not yet seen.\n",
    "eval_result = model.evaluate(input_fn=input_test)\n",
    "\n",
    "# The evaluation returns a Python dictionary. The \"average_loss\" key holds the\n",
    "# Mean Squared Error (MSE).\n",
    "average_loss = eval_result[\"average_loss\"]\n",
    "print(\"Average loss: %s\" % average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run a couple of predictions. (Ignore the `QueueRunner` warning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some prediction input.\n",
    "# input data element items correspond to TRAIN_FEATURE_NAMES list, i.e.:\n",
    "# [curb-weight, highway-mpg]\n",
    "prediction_input = [[2000, 30], [3000, 40]]\n",
    "\n",
    "def predict_input_fn():\n",
    "  def decode(x):\n",
    "      x = tf.split(x, 2) # Need to split into our 2 features\n",
    "      # When predicting, we don't need (or have) any labels\n",
    "      return dict(zip(TRAIN_FEATURE_NAMES, x)) # Then build a dict from them\n",
    "  # The from_tensor_slices function will use a memory structure as input\n",
    "  dataset = tf.contrib.data.Dataset.from_tensor_slices(prediction_input)\n",
    "  dataset = dataset.map(decode)\n",
    "  iterator = dataset.make_one_shot_iterator()\n",
    "  next_feature_batch = iterator.get_next()\n",
    "  return next_feature_batch, None # In prediction, we have no labels\n",
    "\n",
    "predict_results = model.predict(input_fn=predict_input_fn)\n",
    "\n",
    "# Print the prediction results.\n",
    "print(\"\\nPrediction results:\")\n",
    "for i, prediction in enumerate(predict_results):\n",
    "  print(\"i %s, prediction %s\" % (i, prediction))\n",
    "  msg = (\"Curb weight: {: 4d}lbs, \"\n",
    "         \"Highway: {: 0d}mpg, \"\n",
    "         \"Prediction: ${: 9.2f}\")\n",
    "  msg = msg.format(prediction_input[i][0], prediction_input[i][1],\n",
    "                   PRICE_NORM_FACTOR*prediction[\"predictions\"][0])\n",
    "  print(\"    \" + msg)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how your training went, start up TensorBoard as follows in a new terminal window, pointing it to the MODEL_DIR. (If you get a 'not found' error, make sure you've activated your virtual environment in that new window):\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir=<model_dir>\n",
    "```\n",
    "Or run the following (select `Kernel` > `Interrupt` from the menu when you're done):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=$MODEL_DIR"
   ]
  }
 ],
 "metadata": {
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
